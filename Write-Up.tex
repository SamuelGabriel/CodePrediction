\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xspace}
\usepackage{fontspec}
\usepackage{siunitx}
\usepackage{alltt}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\begin{document}
%We will be looking for: 
% 	clear writing and good presentation
% 	how well you deliver on the sections on the previous slide
% 	any new or novel contributions (however minor)


\newcommand{\ggg}[1]{\textcolor{gray}{#1}}
\newcommand{\lmatt}{LMAtt\xspace}
\newcommand{\spn}{SPN\xspace}

\title{%
R252 Write-Up\\[1mm]
\large \textit{Learning Code Suggestion with a Sparse Pointer Network (SPN)}
}
\author{Samuel G. MÃ¼ller (sgm48)}
\maketitle

\section{Introduction}
I reimplemented Sparse Pointer Networks (SPN) \citep{bhoopchand2016learning} on learning a language model (LM) with copy functionality for Python code, adapted it to work with Java and improved upon it by making the model more powerful and by specializing it for Java by considering Java types. In figure \ref{task} an illustration of the task of an LM for code can be found.
\begin{figure}
\centering
\includegraphics[scale=0.7]{figures/lm_task}
\caption{An illustration of the task of an LM on code. In this case the LM would predict the next token after the dot.} 
\label{task}
\end{figure}
\section{Reimplementation}
This section describes my exact implementation of the language model architectures that where compared by \cite{bhoopchand2016learning} for next-token prediction in code.
\label{reimplementation}
\subsection{SPN}
In this section I will describe the SPN architecture. An SPN can be thought of as having three parts. (i) An LSTM with a fully connected layer that maps the hidden state to the the vocabulary. (ii) A copy mechanism that is based on an attention distribution over the previous identifiers. (iii) Lastly, it has a mechanism to combine the prediction from (i) the LSTM and (ii) the copy mechanism.
The copy mechanism has a memory of hidden states $M_t = [h_{ide_t(1)}\dots h_{ide_t(k)}] \in \mathbb{R}^{d \times k}$ of the LSTM at the positions of the previous $k$ identifiers, here $ide_t(i)$ returns the $i$th previous identifier. We compute a distribution $\alpha$ over the previous identifiers with
\begin{align}
	G_{t} &= \text{tanh}(W^M M_{t} + (W^hh_t)1^\top_k)       &&\in \mathbb{R}^{d\times k}\\
	\alpha_{t} &= \text{softmax}(w^\top G_{t})                                  &&\in \mathbb{R}^{1\times k}
\end{align}
where $h_i \in \mathbb{R}^d$ for any $i$ is the hidden states of the LSTM and $t$ is the current time step and $W^M, W^h \in \mathbb{R}^{k \times k}$ as well as $w \in \mathbb{R}^k$ are trainable parameters. $1_k$ is the $k$-dimensional  vector of only ones.
This yields a sparse distribution $i_t$ over the vocabulary $V$ that is defined as
\begin{align}
{i}_t[i] &=
\sum_{j \in \{1 \dots k\}}[m_t[j] = i]\alpha_t[j] &&\in \mathbb{R} \label{copy_distribution}
\intertext{%
where $m_t[j]$ is the index of the input token in step $ide_t(j)$ which yields the hidden state $M_t[j]$ and $i \in \{1\dots |V|\}$.
Besides the distribution $i_t$ of the copy mechanism (ii) there is the distribution $y_t$ from the LSTM (i), that is defined as
}
	y_t &= \text{softmax}(W^Vh_t + b^V) &&\in \mathbb{R}^{|V|} \label{y_t}
\intertext{%
where $W^V \in \mathbb{R}^{|V| \times d}$ and $b^V \in \mathbb{R}^{|V|}$ are trainable parameters. $y_t$ and $i_t$ are combined using a weight $\lambda_t$. $\lambda_t$ depends on the context $c_t$ that is defined as $c_t = M_t\alpha_t^T$, the average of the hidden states in the memory weighted by $\alpha_t$, and the representation of the last input token of the LSTM $x_t$. $\lambda_t$ can now be defined as}
	h_t^\lambda &= \left[h_t; x_t; c_t\right]  &&\in \mathbb{R}^{3d}\\
	\lambda_t &= \text{softmax}(W^\lambda h_t^\lambda +b^\lambda) &&\in \mathbb{R}^{2}
\intertext{%
where $W^\lambda \in \mathbb{R}^{2 \times 3d}$ and $b^\lambda \in \mathbb{R}^2$ are trainable parameters.
With $\lambda_t$ as weight at hand we can now define our final next-token distribution as}
	y^*_t &= [y_t, i_t]\lambda_t &&\in \mathbb{R}^{|V|}. \label{final_mix}
\end{align}

\subsection{Baselines}
\subsubsection{LSTM with Attention (\lmatt)}
\label{lmatt}
This baseline uses most of the components of the \spn, but instead of using $\alpha_t$ as copy distribution, it only uses $c_t$ to predict the next token with the distribution $y'_t$, defined as
\begin{align}
	{n}_t &= \text{tanh}\left(W^A \begin{bmatrix}
		{h}_t \\
		{c}_t
	\end{bmatrix}
		\right) && \in \mathbb{R}^d \label{eq:ch5-att-combine}\\
	y'_t &= \operatorname{softmax}(W^V n_t + b^V) && \in \mathbb{R}^{|V|} \label{eq:ch5-lm-softmax-2}
\end{align}
where $W^A \in \mathbb{R}^{d \times 2d}$, $W^V \in \mathbb{R}^{|V| \times d}$ and ${b}^V \in \mathbb{R}^{|V|}$ are trainable parameters.
Another change to the model above for \lmatt is that $M_t$ does not include the hidden states of the LSTM at the $k$ previous identifiers, but instead holds the previous $k$ hidden states of any tokens that is $M_t = [h_{t-k}\dots h_{t-1}]$.
\subsubsection{LSTM}
The LSTM baseline is to just use $y_t$ as defined in equation \ref{y_t} as next-token distribution. This yields a very simple LM that does not use any previous hidden state, but predicts the next character solely based on the current hidden state.

\subsection{Implementation and Training Details}
All tokens were presented to the LSTM as learned embeddings of size $d$ and the initial hidden state of the LSTM is set to be all zeros. $d$ was chosen to be 200. $k$ was set to 30 for all models. Whole files were fed consecutively, so that we initialize the LSTM only at the beginning of each file.
During training the gradient of every parameter element was clipped to be maximally 5 times the norm of the flattened vector of all parameter\textquotesingle{s} elements as described by \cite{pascanu2012difficulty}.  Dropout was applied to the inputs of the LSTM during training with a keep rate of 0.9. As optimizer an SGD with an initial learning rate of 0.7 and a learning decay factor of 0.9 per epoch was used. All parameters were uniformly initialized between -0.05 and 0.05. During training all copy probabilities, defined in equation \ref{copy_distribution}, were increased by \num{1e-10}, so that the cross entropy, the sum of the logarithms of these probabilities, is defined even for situations in which it is not possible to copy.

\subsection{Dataset Preparation}
\subsubsection{Dataset Normalization}
\label{normalization}
The dataset was normalized by replacing every identifier with a string indicating what kind of identifier it is (class, function, parameter, argument or local variable) and a random number between 1 and 3000 that is unique in the file.
\subsubsection{Dataset Split}
The provided dataset was split into training, validation and test set on the package level, which makes sure we have a minimal information leakage and thus more robust results. The test set was only used to run experiments exactly ones for each set-up. The training set has a size of 1.12 GB, the validation set has a size of 24.7 MB and the test set has a size of 35.5 MB.

\section{Comparison}
In this section I will detail the differences between the implementation by \cite{bhoopchand2016learning} and mine, as well as attention sharing, a model extension, and a type-specific dataset normalization.
\subsection{Implementation Differences}
My implementation differs from the model described in \cite{bhoopchand2016learning} in two main ways. (i) They removed comments and replaced numericals with a single 	\textit{\$NUM\$} token, both of which I did not. Also, they replaced words with an out-of-vocabulary token if they appeared less than three times, while I used a fixed vocabulary size of 5000. Lastly, they normalized identifiers to be unique in their scope, while my normalization targeted file-level uniqueness, which for Java in most cases means class-level uniqueness. (ii) While \cite{bhoopchand2016learning} describe that they use a pseudo softmax distribution for their copy distribution $i_t$ both during training and inference, I only used it at training time, where it is necessary.
The only hyper parameter that differs is the memory size $k$. I was able to use a $k$ of 30 for all my experiments, while \cite{bhoopchand2016learning} used a $k$ of 20 for the \spn and the \lmatt and added another experiment with a $k$ of 50 for the \lmatt.
\subsection{Extensions}
\label{extensions}
\subsubsection{Incorporating Types}
\label{incorporating_types}
For Python identifiers it is non-trivial to determine their type statically, since Python has a dynamic typing system and so variables might change type over time.
In Java on the other hand we have a static typing system and so one can use types to make better predictions of the next token. I did not incorporated types into my model directly, but left the model as general as possible, and instead changed the normalization approach to represent types.
The normalization approach, as presented by \cite{bhoopchand2016learning},  normalizes variables without taking into account of what type these variables are by replacing an argument of type \textit{String} for example with the token \textit{Argument123}. My typed approach to normalization on the other hand is to replace the argument in the example with e.g.\ the token \textit{Argument-String123}.\\
In general the typed normalization approach replaces all identifiers, besides class names and function names, with anonymous identifiers of the form <Identifier Type>-<Java Type><Number>, where the appended number is chosen randomly from a type-specific range. I treated all built-in types, as a single type, and type parameters were ignored for generic types. Both of these design decisions were made so that the number of different types does not explode, since a larger number of types also means that the normalized data has a larger vocabulary. To decrease the vocabulary size even further the range from which the appended number is drawn is chosen to be the minimal range for each type that allows all normalized identifiers to be unique in their files.

\subsubsection{Attention Sharing}
\label{attshar}
As described in section \ref{reimplementation} the \spn already has a distribution over previous hidden states of the LSTM and also already computes the weighted average $c_t$, so the logical next step would be to use $c_t$ for the next-token prediction of the LSTM, as \cite{See_2017} did in their model for abstractive summarization.
This can easily be done by changing the definition of $y^*_t$ in the \spn to not be a mixture of $y_t$ and $i_t$, but of $y'_t$, defined in section \ref{lmatt}, and $i_t$. That means one replaces the final distribution of the \spn from equation \ref{final_mix} with
\begin{align}
	y^*_t = [y'_t, i_t]\lambda_t &&\in \mathbb{R}^{|V|}.
\end{align}
A more visual explanation of the models described, including the attention sharing, can be found in figure \ref{dependencies}.
Attention sharing can be used with the attention being restricted to identifiers or without this restriction. The version without attention restriction though is much simpler and in experiments the performance could only be improved by a very small margin, thus I only consider the non-restricted case here. 

\begin{figure}
\centering
\includegraphics[scale=0.7]{figures/Model-Dependencies}
\caption{A schematic drawing of the dependencies between different components of the models described. Dependencies of the LSTM: blue; \lmatt: blue + magenta + green; \spn: blue + magenta + orange; and lastly the \spn with shared attention uses all connections.} 
\label{dependencies}
\end{figure}

\section{Evaluation}
%Evaluation in the form of hypothesis, experiment (method), results, conclusion.  Example hypothesis: performance relative to original paper

\subsection{\spn Performance in Comparison to Baselines}
\cite{bhoopchand2016learning} compare the \spn with the \lmatt and an LSTM. In their results the \lmatt with an attention window of 50 steps has the highest accuracy, the \spn has the lowest perplexity and the LSTM performs worst among these in both metrics. These results could be reproduced in my experiments. The exact results can be found in table \ref{spn_perf}. In our experiments the \spn did not perform as well as for \cite{bhoopchand2016learning} compared to the baselines: While I observed a 4.34 \% higher accuracy of \lmatt relative to the \spn\textquotesingle{s} accuracy, they only see an improvement of 0.38\% and the improvement upon the LSTM\textquotesingle{s} perplexity they observe is of 31.00\%, I could only observe an improvement of 2.85\%. This might be due to the fact that the data worked with has a different distribution, they work with Python code and I work with Java code, but it could also be due to the fact that they chose their data to support their thesis that the \spn is a good model. The second claim, as unlikely as it might be, is supported by the fact that \cite{bhoopchand2016learning} did not use a standard dataset but instead built one themselves. A third possibility might be that my implementation of the \spn differs from theirs.
\begin{table}
\centering
	\begin{tabular}{c | c | c | c}
		  & LSTM & \lmatt & \spn\\\hline
		Test Perplexity & 39.58 & 116.44 & 38.45	\\\hline
		Test Accuracy & 60.43 \% & 63.64 \% & 60.99 \%
	\end{tabular}
\label{spn_perf}
\caption{This table shows the performance of the \spn and the baselines used by \cite{bhoopchand2016learning}, i.e.\ a \lmatt and a LSTM.}
\end{table}


\subsection{Can the \spn be improved by sharing the attention weights between the copy mechanism and the LSTM?}
\label{attshar_results}
While the \spn is a powerful model, it does not make use of the attention weights computed to generate better next-token predictions using the LSTM, therefore the thesis for this experiment is that the \spn model and the \lmatt model can be improved by merging them, as described in section \ref{attshar}. While this definitely makes the model more powerful theoretically, due to a new connection, it also increases the number of parameters. The size does not increase too much on the other hand, since we only introduce one new layer to the \spn and therefore grow the number of parameters by just 2.4 \%.\\
Sharing attention improves the \spn clearly, as can be seen in \ref{attshar_table}, and it does so without restricting its attention to identifiers, the main novelty in \cite{bhoopchand2016learning}. It does, on the other hand, outperform the \lmatt in perplexity only.
\begin{table}
\centering
	\begin{tabular}{c | c | c}
		Metric \textbackslash{} Model & \spn & Attention Sharing\\\hline
		Test Perplexity & 38.45 & 28.73	\\\hline
		Test Accuracy & 60.99 \% & 62.63 \%
	\end{tabular}
\label{attshar_table}
\caption{This table shows the performance of the \spn with and without attention sharing.}
\end{table}


\subsection{The Effects of a Typed Corpus}
In section \ref{incorporating_types} a method is introduced that does not change the model but still lets it depend more directly on types of identifiers. This is done by changing the identifier normalization.\\
Even though this is a very simple method to incorporate types into the model, I still expect that the accuracy increases when training the same model on the typed dataset, since the type of a variable drastically changes the places in code in which it can be used. It is only fair to compare accuracy here, since this metric, unlike perplexity, cannot be artificially increased by increasing the number of unknown tokens in the dataset. I trained the \spn on the typed corpus and compared its performance with the performance of the \spn trained on the non-typed corpus. You can find the results in table \ref{typed_exp}. The accuracy could be improved by adding types and the frequency of out-of-vocabulary tokens the model sees could be decreased from 12.4\% on the non-typed corpus to 10.3\%, even though the number of tokens in the corpus was slightly increased from \num{1.10e5} to \num{1.31e5} by the typed normalization.

\begin{table}
\centering
	\begin{tabular}{c | c | c }
		Metric \textbackslash{} Corpus & Non-Typed & Typed \\\hline
		Test Accuracy & 60.99 \% & 62.63 \%
	\end{tabular}
\caption{This table compares the performance of the \spn trained on the non-typed corpus, described in section \ref{normalization}, and trained on the typed corpus, described in section \ref{incorporating_types}.}
\label{typed_exp}
\end{table}

\subsection{Qualitative Analysis of an Example Prediction}
As an example to analyze I chose a two line sequence, that refers to the same identifier twice, randomly from the test corpus.
Below the example to be analyzed can be seen:
\begin{verbatim}
localvariable91 = new hashmap<>();
localvariable91.put(<unk>, ...
\end{verbatim}
In this code snippet a previously defined variable is reassigned to a newly initialized hashmap and in the next line an entry is added to that hashmap, where the key is an out-of-vocabulary token and the value is omitted.
We are going to analyze teacher-forced predictions of an LSTM, an \spn, an \spn with attention sharing and an \spn with the typed corpus. Gray tokens were predicted with less than 75\% confidence.
The LSTM predicts the following:
\begin{alltt}
\ggg{assertthat} = \ggg{new} <unk><>();
\ggg{for}.\ggg{<unk>}(\ggg{<unk>,} ...
\end{alltt}
 While the \spn predicts:
\begin{alltt}
\ggg{assertthat} = \ggg{<unk>} <unk><>();
\ggg{<unk>} = put(\ggg{<unk>,} ...
\end{alltt}
Both predict the last tokens of the initialization of the hashmap well, which are a common pattern in Java. While the LSTM correctly predicts that in line 2 we want to call a member function of the above defined variable, the \spn goes with the very nonintuitive prediction of a reassignment of the just previously defined variable. The \spn seems to have some understanding of the type of the variable, since it correctly predicts what member function is called.\\
The \spn can be improved with attention sharing, as can be seen in the prediction of an \spn with attention sharing below:
\begin{alltt}
assertthat = \ggg{<unk>} <unk><>();
\ggg{localvariable91.}put(<unk>, ...
\end{alltt}
It does not only have an idea of the type of the variable, since it correctly predicts the member function called in the second line, but it also predicts that we want to call a member-function of the previously defined variable.
This can be even further improved by making the types of variables more accessible to the \spn. The same example from above has the following form in the typed corpus:
\begin{verbatim}
localvariable-map41 = new hashmap<>();
localvariable-map41.put(<unk>, ...
\end{verbatim}
The \spn trained on this dataset predicts the following:
\begin{alltt}
\ggg{assertthat = new hashmap}<>();
\ggg{localvariable-map41}.put(<unk>, ...
\end{alltt}
It predicts the whole sequence, besides the first token, but it has a lot of tokens with low confidence.\\
Generally these predictions exemplify the strengths of the extensions proposed in this work, to incorporate types into the \spn and to share the attention weights in the \spn. 

\section{Reflection}
%Reflection on your plan: what was slower/faster than expected & why?
\subsection{Getting Started}
To get to the first implementation of the SPN and the \lmatt was quite quick, since \cite{bhoopchand2016learning} published their code and I could reuse it after updating it to the Python and TensorFlow version that I am using.
\subsection{Dataset Preparation}
The dataset preparation, especially the normalization procedure, was not very easy to get done, but at least it was a pretty deterministic task that just needed some thought and optimization to work and do so in a matter of hours and not days.
\subsection{Exact Reimplementation}
After the initially constant progress I reached a long progress plateau, due to two slight differences in my implementation compared to theirs.
\paragraph{Batching} First, it did make a difference that I did not feed the model whole files during training but only extracts of 100 tokens. It was not easy to feed longer sequences because of the memory limitations on GPUs, that is why I needed to change the batching mechanism to compose batches of files with similar lengths and then feed batches of 100 tokens from each file to the model at each step. This way I was able to train on very long consecutive sequences without a too big efficiency loss.
\paragraph{Settings} Second, my hyper-parameter settings and my architecture were slightly different from theirs. I computed the attention weights $\alpha_t$ not over the hidden state of the LSTM, but instead over the token embeddings. In hindsight it makes a lot of sense that this makes a significant difference, since the embeddings of the normalized identifiers should not give away much more than the obvious information that this is an identifier of some type, while the hidden state also entails the usage and possibly the circumstances of its definition, e.g.\ if it is a loop variable. Lastly, I used a too small memory size $k$.
\\\\
These problems were intensified by the fact, that training the model on the full dataset does take very long.
\subsection{Extensions}
After finally having a version of the \spn and the \lmatt that clearly outperform the LSTM, I could begin working on the extensions detailed in section \ref{extensions}. While the attention sharing and the typed normalization were working in only a few iterations, since the model and the normalization script were easily extensible, there was another extension I put time and thought into that did not produce the results I hoped for. This extension should teach the model to copy more, by masking the LSTM prediction during training whenever it was possible to copy the correct next token.

\subsection{Experiments}
Undertaking most of the experiments was straight forward, since I already had trained models that I could just evaluate on the test set. The only experiments that were harder than I thought were the ones were I needed to change the evaluation procedure to get more information out of the model, such as the number of identifiers, out-of-vocabulary tokens and the prediction weights.

\section{Future Work}
The typed normalization is well-defined for Java versions prior to Java 10, but Java 10 introduces type inference, which yields situations in which a variable token appears before it's type definition and thus the language model on the typed corpus would suffer an information leakage. This should be addressed so that the language model\textquotesingle{s} objective is as close as possible to next token predictions in editors.


\bibliography{main}
\bibliographystyle{agsm}
\end{document}
